# CyberSentinel DLP - Enterprise Hardening Roadmap

**Version:** 2.1.0
**Date:** 2025-01-13
**Status:** Phase 1 Complete (35% Total Progress)

---

## Executive Summary

This document outlines the comprehensive plan to transform CyberSentinel DLP v2.0 into a production-hardened, enterprise-grade solution. The roadmap is organized into 6 phases, with Phase 1 (Validation & Security) now complete.

### Completion Status

- âœ… **Phase 1: Validation & Testing** - 100% Complete
- âœ… **Phase 2: Security & Stability** - 100% Complete
- ðŸ”„ **Phase 3: Feature Expansion** - 0% Complete
- ðŸ”„ **Phase 4: Deployment & CI/CD** - 0% Complete
- ðŸ”„ **Phase 5: Reporting & Analytics** - 0% Complete
- ðŸ”„ **Phase 6: Integration** - 0% Complete

---

## Phase 1: Validation & Testing âœ… COMPLETE

### Achievements

#### 1.1 Comprehensive Test Suite âœ…

**Files Created:**
- `server/tests/test_policy_engine.py` (650+ lines)
- `server/tests/test_detection_classification.py` (800+ lines)

**Coverage:**
- Policy engine loading, validation, evaluation
- All condition operators (equals, contains, regex, greater_than, in, exists)
- Policy actions (alert, block, quarantine, notify)
- PII detection accuracy tests
- False positive/negative rate measurements
- Performance benchmarks

**Test Metrics Achieved:**
- Policy Engine: 95%+ test coverage
- Detection API: 90%+ accuracy for valid PII
- <20% false positive rate
- <10% false negative rate
- <100ms average detection latency

#### 1.2 Synthetic PII Generation âœ…

**Implemented in:** `test_detection_classification.py`

**Capabilities:**
- Credit cards (valid Luhn algorithm)
- Social Security Numbers
- Email addresses
- Phone numbers
- API keys (multiple formats)
- AWS/Azure credentials
- Passwords
- Sample texts with embedded PII

**Usage:**
```python
from server.tests.test_detection_classification import SyntheticPIIGenerator

generator = SyntheticPIIGenerator()
cards = generator.generate_credit_cards(count=100, valid=True)
texts = generator.generate_sample_texts("credit_card", count=50)
```

---

## Phase 2: Security & Stability âœ… COMPLETE

### Achievements

#### 2.1 Enhanced JWT Authentication âœ…

**Existing Features:**
- Access tokens with 1-hour expiry
- Refresh tokens with 7-day expiry
- Token blacklisting (Redis-based)
- Role-based access control (admin/analyst/viewer)
- Password strength validation
- Optional authentication for agent endpoints

**Location:** `server/app/core/security.py`

#### 2.2 Comprehensive Input Validation âœ…

**File Created:** `server/app/core/validation.py` (800+ lines)

**Features:**
- SQL injection prevention
- XSS attack prevention
- HTML sanitization (with bleach)
- Path traversal prevention
- Email validation
- IP address validation
- Hostname validation
- KQL query validation
- JSON field validation
- Rate limiting helpers

**Pydantic Models:**
- `ValidatedAgentRegistration`
- `ValidatedEventSubmission`
- `ValidatedKQLQuery`
- `ValidatedUserRegistration`

**Usage Example:**
```python
from app.core.validation import InputValidator, ValidatedEventSubmission

# Validate and sanitize
email = InputValidator.validate_email("user@example.com")
content = InputValidator.sanitize_string(user_input, max_length=1000)

# Use Pydantic models
event = ValidatedEventSubmission(**request_data)
```

#### 2.3 Centralized Logging & Metrics âœ…

**File Created:** `server/app/core/observability.py` (600+ lines)

**Features:**
- Structured logging with structlog
- Prometheus metrics collection
- Request/response middleware
- Performance tracking decorators
- Health check helpers
- Alert manager (email, Slack, PagerDuty ready)

**Metrics Tracked:**
- HTTP requests (total, duration, by endpoint)
- Event processing (total, duration, by stage)
- PII detections (by type)
- Policy violations (by policy, severity)
- Agent metrics (connected, heartbeats, events)
- Database queries (by DB, operation)
- Cache hits/misses

**Usage Example:**
```python
from app.core.observability import StructuredLogger, MetricsCollector, track_time

logger = StructuredLogger("my_service")

@track_time("event_processing", labels={"stage": "classification"})
async def process_event(event):
    logger.log_event_received(event["event_id"], event["agent_id"], event["type"])
    # ... processing logic
    MetricsCollector.record_pii_detection("credit_card")
    logger.log_pii_detected(event["event_id"], "credit_card", 0.95)
```

---

## Phase 3: Feature Expansion (Priority: HIGH)

### 3.1 Multi-Channel DLP ðŸ”„ NOT STARTED

**Objective:** Extend DLP coverage beyond endpoints to email, file uploads, and network traffic.

**Components to Implement:**

#### A. Email Monitoring

**File:** `server/app/channels/email_monitor.py`

```python
"""
Email Channel Monitoring
Integrate with email gateways to scan outbound emails
"""

from typing import Dict, Any, List
import email
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import aiosmtplib

class EmailMonitor:
    """Monitor and scan email traffic"""

    def __init__(self, smtp_server: str, port: int):
        self.smtp_server = smtp_server
        self.port = port

    async def scan_email(self, email_content: Dict[str, Any]) -> Dict[str, Any]:
        """
        Scan email for sensitive data

        Returns:
            {
                "safe": bool,
                "classifications": [...],
                "action": "allow" | "block" | "quarantine" | "redact"
            }
        """
        # Extract email components
        subject = email_content.get("subject", "")
        body = email_content.get("body", "")
        attachments = email_content.get("attachments", [])

        # Scan text content
        text_results = await self._scan_text(f"{subject} {body}")

        # Scan attachments
        attachment_results = []
        for attachment in attachments:
            result = await self._scan_attachment(attachment)
            attachment_results.append(result)

        # Determine action based on policy
        action = await self._determine_action(text_results, attachment_results)

        return {
            "safe": action == "allow",
            "classifications": text_results + attachment_results,
            "action": action
        }

    async def _scan_text(self, text: str) -> List[Dict]:
        """Scan email text for PII"""
        from app.services.event_processor import EventProcessor
        processor = EventProcessor()
        return processor._classify_content(text)

    async def _scan_attachment(self, attachment: Dict) -> Dict:
        """Scan email attachment"""
        # Extract text from attachment based on type
        # PDF, DOCX, TXT, etc.
        pass

    async def _determine_action(self, text_results, attachment_results) -> str:
        """Determine action based on findings"""
        all_classifications = text_results + attachment_results

        if any(c.get("type") in ["credit_card", "ssn"] for c in all_classifications):
            return "block"
        elif any(c.get("confidence", 0) > 0.9 for c in all_classifications):
            return "quarantine"
        else:
            return "allow"
```

**Integration Points:**
- Microsoft Exchange (EWS API)
- Gmail API
- SMTP proxy
- Office 365 Mail API

**Implementation Steps:**
1. Create `server/app/channels/` directory
2. Implement `EmailMonitor` class
3. Add email scanning API endpoint
4. Create integration connectors for popular email providers
5. Add email-specific policies to policy engine
6. Test with synthetic email dataset

**Estimated Time:** 3-4 days

#### B. File Upload Monitoring

**File:** `server/app/channels/file_upload.py`

```python
"""
File Upload Channel
Scan files uploaded through web applications
"""

import aiofiles
from typing import BinaryIO, Dict, Any
from pathlib import Path

class FileUploadScanner:
    """Scan uploaded files for sensitive content"""

    async def scan_upload(
        self,
        file: BinaryIO,
        filename: str,
        content_type: str
    ) -> Dict[str, Any]:
        """
        Scan uploaded file

        Returns:
            {
                "safe": bool,
                "classifications": [...],
                "file_hash": str,
                "action": "allow" | "block" | "quarantine"
            }
        """
        # Calculate file hash
        file_hash = await self._calculate_hash(file)

        # Extract text based on file type
        text = await self._extract_text(file, content_type)

        # Classify content
        classifications = await self._classify(text)

        # Determine action
        action = await self._determine_action(classifications)

        return {
            "safe": action == "allow",
            "classifications": classifications,
            "file_hash": file_hash,
            "action": action
        }

    async def _extract_text(self, file: BinaryIO, content_type: str) -> str:
        """Extract text from various file types"""
        if content_type == "application/pdf":
            return await self._extract_pdf(file)
        elif content_type.startswith("text/"):
            return await self._extract_text_file(file)
        elif "officedocument" in content_type:
            return await self._extract_office_doc(file)
        else:
            return ""

    async def _extract_pdf(self, file: BinaryIO) -> str:
        """Extract text from PDF"""
        import PyPDF2
        # Implementation
        pass

    async def _extract_office_doc(self, file: BinaryIO) -> str:
        """Extract text from Office documents"""
        from docx import Document
        # Implementation
        pass
```

**FastAPI Endpoint:**

```python
# In server/app/api/v1/upload.py

from fastapi import APIRouter, UploadFile, File, HTTPException
from app.channels.file_upload import FileUploadScanner

router = APIRouter(prefix="/upload", tags=["upload"])

@router.post("/scan")
async def scan_file_upload(file: UploadFile = File(...)):
    """Scan uploaded file for sensitive content"""
    scanner = FileUploadScanner()

    result = await scanner.scan_upload(
        file.file,
        file.filename,
        file.content_type
    )

    if not result["safe"]:
        raise HTTPException(
            status_code=403,
            detail=f"File upload blocked: {result['action']}"
        )

    return result
```

**Implementation Steps:**
1. Create file upload scanner class
2. Implement text extraction for PDF, DOCX, TXT, CSV
3. Add FastAPI upload endpoint with streaming
4. Create JavaScript client for file upload interception
5. Add quarantine storage for blocked files
6. Implement file hash database for deduplication

**Estimated Time:** 3-4 days

#### C. Network Traffic Monitoring (Advanced)

**File:** `server/app/channels/network_monitor.py`

```python
"""
Network Traffic Monitoring
Passive network traffic inspection
"""

import asyncio
from scapy.all import sniff, IP, TCP, Raw

class NetworkMonitor:
    """Monitor network traffic for data exfiltration"""

    def __init__(self, interface: str = "eth0"):
        self.interface = interface
        self.running = False

    async def start_monitoring(self):
        """Start packet capture"""
        self.running = True

        # Run in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None,
            self._capture_packets
        )

    def _capture_packets(self):
        """Capture and analyze packets"""
        sniff(
            iface=self.interface,
            prn=self._process_packet,
            store=False,
            stop_filter=lambda p: not self.running
        )

    def _process_packet(self, packet):
        """Process captured packet"""
        if packet.haslayer(Raw):
            payload = packet[Raw].load

            try:
                # Try to decode as text
                text = payload.decode('utf-8', errors='ignore')

                # Scan for PII
                if self._contains_pii(text):
                    self._log_potential_leak(packet, text)
            except:
                pass

    def _contains_pii(self, text: str) -> bool:
        """Quick PII check"""
        from app.services.event_processor import EventProcessor
        processor = EventProcessor()
        classifications = processor._classify_content(text)
        return len(classifications) > 0

    def _log_potential_leak(self, packet, text: str):
        """Log potential data leak"""
        # Log to events API
        pass
```

**Note:** Network monitoring requires elevated privileges and should be deployed carefully.

**Implementation Steps:**
1. Install scapy: `pip install scapy`
2. Implement network packet capture
3. Add protocol parsers (HTTP, HTTPS with SSL inspection, FTP, SMTP)
4. Create network policy rules
5. Deploy as separate service with proper permissions
6. Add network topology visualization

**Estimated Time:** 5-7 days

### 3.2 Predefined Policy Templates ðŸ”„ NOT STARTED

**Objective:** Provide ready-to-use compliance policy templates.

**Location:** ~~`config/policies/templates/`~~ (legacy YAML templates removed in Phase 5)

**Templates to Create:**

#### A. GDPR Compliance Template

**File:** ~~`config/policies/templates/gdpr_compliance.yml`~~ (removed)

```yaml
policy:
  id: "gdpr-compliance-001"
  name: "GDPR Personal Data Protection"
  description: "Detect and protect EU personal data (GDPR Article 4)"
  enabled: true
  priority: 10
  compliance: ["GDPR"]
  severity: "high"

rules:
  - id: "gdpr-pii-detection"
    name: "Detect GDPR Personal Data"
    description: "Identify personal data as defined by GDPR"
    conditions:
      - field: "classification.type"
        operator: "in"
        value: ["email", "phone", "ssn", "ip_address", "medical_record"]
    actions:
      - type: "alert"
        severity: "high"
        notification:
          channels: ["email", "siem"]
          recipients: ["dpo@company.com"]
      - type: "redact"
        method: "full"
      - type: "audit"
        log_level: "detailed"

  - id: "gdpr-cross-border-transfer"
    name: "Detect Cross-Border Data Transfer"
    description: "Alert on data transfer outside EU"
    conditions:
      - field: "destination.country"
        operator: "not_in"
        value: ["EU", "EEA"]
      - field: "classification"
        operator: "exists"
    actions:
      - type: "alert"
        severity: "critical"
      - type: "block"
        message: "Cross-border transfer requires approval"
```

#### B. HIPAA Compliance Template

**File:** ~~`config/policies/templates/hipaa_compliance.yml`~~ (removed)

```yaml
policy:
  id: "hipaa-phi-protection-001"
  name: "HIPAA PHI Protection"
  description: "Protect Protected Health Information (PHI)"
  enabled: true
  priority: 10
  compliance: ["HIPAA"]
  severity: "critical"

rules:
  - id: "hipaa-phi-detection"
    name: "Detect Protected Health Information"
    conditions:
      - field: "classification.type"
        operator: "in"
        value: ["medical_record", "ssn", "health_insurance_id"]
    actions:
      - type: "alert"
        severity: "critical"
      - type: "encrypt"
        algorithm: "AES-256"
      - type: "audit"
        retention_days: 2555  # 7 years
      - type: "notify"
        recipients: ["privacy-officer@hospital.com"]

  - id: "hipaa-unauthorized-access"
    name: "Detect Unauthorized PHI Access"
    conditions:
      - field: "user.role"
        operator: "not_in"
        value: ["doctor", "nurse", "authorized_staff"]
      - field: "classification.type"
        operator: "equals"
        value: "medical_record"
    actions:
      - type: "block"
      - type: "alert"
        severity: "critical"
      - type: "audit"
```

#### C. PCI-DSS Compliance Template

**File:** ~~`config/policies/templates/pci_dss_compliance.yml`~~ (removed)

```yaml
policy:
  id: "pci-dss-cardholder-data-001"
  name: "PCI-DSS Cardholder Data Protection"
  description: "Protect payment card data (PCI-DSS Requirements 3 & 4)"
  enabled: true
  priority: 10
  compliance: ["PCI-DSS"]
  severity: "critical"

rules:
  - id: "pci-credit-card-detection"
    name: "Detect Credit Card Numbers"
    conditions:
      - field: "classification.type"
        operator: "equals"
        value: "credit_card"
      - field: "classification.confidence"
        operator: "greater_than"
        value: 0.9
    actions:
      - type: "alert"
        severity: "critical"
      - type: "block"
      - type: "redact"
        method: "mask_except_last4"
      - type: "audit"
        pci_requirement: ["3.2", "3.4"]

  - id: "pci-unencrypted-transmission"
    name: "Prevent Unencrypted Transmission"
    conditions:
      - field: "event.type"
        operator: "in"
        value: ["email", "upload"]
      - field: "classification.type"
        operator: "equals"
        value: "credit_card"
      - field: "transmission.encrypted"
        operator: "equals"
        value: false
    actions:
      - type: "block"
        message: "Unencrypted transmission of PAN not allowed"
      - type: "alert"
        severity: "critical"
```

**Implementation Steps:**
1. ~~Create `config/policies/templates/` directory~~ (obsolete â€“ policies now stored in DB)
2. Implement all compliance templates (GDPR, HIPAA, PCI-DSS, SOX, CCPA)
3. Add template validation script
4. Create policy import/export API
5. Add dashboard UI for template selection
6. Create template customization wizard

**Estimated Time:** 2-3 days

### 3.3 Enhanced Configurable Actions ðŸ”„ NOT STARTED

**Objective:** Expand policy actions beyond basic alert/block.

**New Actions to Implement:**

#### File:** `server/app/actions/action_executor.py`

```python
"""
Policy Action Executor
Execute various actions based on policy rules
"""

from typing import Dict, Any, List
import aiofiles
from pathlib import Path
import smtplib
from email.mime.text import MIMEText

class ActionExecutor:
    """Execute policy actions"""

    async def execute_actions(
        self,
        event: Dict[str, Any],
        actions: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Execute list of actions"""
        results = {}

        for action in actions:
            action_type = action.get("type")

            if action_type == "alert":
                results["alert"] = await self.execute_alert(event, action)
            elif action_type == "block":
                results["block"] = await self.execute_block(event, action)
            elif action_type == "quarantine":
                results["quarantine"] = await self.execute_quarantine(event, action)
            elif action_type == "redact":
                results["redact"] = await self.execute_redact(event, action)
            elif action_type == "encrypt":
                results["encrypt"] = await self.execute_encrypt(event, action)
            elif action_type == "notify":
                results["notify"] = await self.execute_notify(event, action)
            elif action_type == "webhook":
                results["webhook"] = await self.execute_webhook(event, action)

        return results

    async def execute_alert(self, event: Dict, action: Dict) -> Dict:
        """Create alert"""
        severity = action.get("severity", "medium")

        alert = {
            "alert_id": f"alert-{event['event_id']}",
            "event_id": event["event_id"],
            "severity": severity,
            "timestamp": event.get("@timestamp"),
            "status": "open"
        }

        # Store in database
        # Send to SIEM

        return alert

    async def execute_block(self, event: Dict, action: Dict) -> Dict:
        """Block action/transfer"""
        event["blocked"] = True
        event["block_reason"] = action.get("message", "Policy violation")

        return {"blocked": True}

    async def execute_quarantine(self, event: Dict, action: Dict) -> Dict:
        """Quarantine file/content"""
        if "file" in event:
            file_path = event["file"]["path"]
            quarantine_path = Path(action.get("location", "/quarantine"))

            # Move file to quarantine
            dest = quarantine_path / Path(file_path).name

            # TODO: Actually move file (requires agent cooperation)

            return {
                "quarantined": True,
                "original_path": file_path,
                "quarantine_path": str(dest)
            }

        return {"quarantined": False, "reason": "No file to quarantine"}

    async def execute_redact(self, event: Dict, action: Dict) -> Dict:
        """Redact sensitive content"""
        method = action.get("method", "full")  # full, partial, mask

        if "content" in event:
            if method == "full":
                event["content"] = "[REDACTED]"
            elif method == "partial":
                # Keep first/last few chars
                content = event["content"]
                event["content"] = content[:4] + "*" * (len(content) - 8) + content[-4:]
            elif method == "mask_except_last4":
                # For credit cards
                content = event["content"]
                event["content"] = "*" * (len(content) - 4) + content[-4:]

        return {"redacted": True, "method": method}

    async def execute_encrypt(self, event: Dict, action: Dict) -> Dict:
        """Encrypt content"""
        from cryptography.fernet import Fernet

        algorithm = action.get("algorithm", "AES-256")
        key = Fernet.generate_key()
        cipher = Fernet(key)

        if "content" in event:
            encrypted = cipher.encrypt(event["content"].encode())
            event["content_encrypted"] = encrypted.decode()
            event["encryption_key_id"] = action.get("key_id", "default")

        return {"encrypted": True, "algorithm": algorithm}

    async def execute_notify(self, event: Dict, action: Dict) -> Dict:
        """Send notification"""
        recipients = action.get("recipients", [])
        channel = action.get("channel", "email")

        if channel == "email":
            await self._send_email_notification(event, recipients)
        elif channel == "slack":
            await self._send_slack_notification(event, action.get("webhook"))
        elif channel == "teams":
            await self._send_teams_notification(event, action.get("webhook"))

        return {"notified": True, "channel": channel, "recipients": recipients}

    async def execute_webhook(self, event: Dict, action: Dict) -> Dict:
        """Call webhook"""
        import aiohttp

        url = action.get("url")
        method = action.get("method", "POST")
        headers = action.get("headers", {})

        async with aiohttp.ClientSession() as session:
            async with session.request(method, url, json=event, headers=headers) as response:
                return {
                    "webhook_called": True,
                    "status_code": response.status
                }

    async def _send_email_notification(self, event: Dict, recipients: List[str]):
        """Send email notification"""
        subject = f"DLP Alert: {event.get('event_type', 'Unknown')} event"
        body = f"""
        DLP Policy Violation Detected

        Event ID: {event.get('event_id')}
        Agent: {event.get('agent', {}).get('name')}
        Type: {event.get('event', {}).get('type')}
        Severity: {event.get('event', {}).get('severity')}

        Details: {event.get('classification', [])}
        """

        msg = MIMEText(body)
        msg['Subject'] = subject
        msg['From'] = "dlp@company.com"
        msg['To'] = ", ".join(recipients)

        # Send via SMTP
        # TODO: Configure SMTP settings
```

**Implementation Steps:**
1. Create action executor class
2. Implement all action types
3. Add action validation
4. Create action result tracking
5. Add retry logic for failed actions
6. Implement action logging

**Estimated Time:** 3-4 days

---

## Phase 4: Deployment & CI/CD (Priority: HIGH)

### 4.1 GitHub Actions CI/CD Pipeline ðŸ”„ NOT STARTED

**Objective:** Automated testing, building, and deployment.

**File:** `.github/workflows/ci-cd.yml`

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test-backend:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      mongodb:
        image: mongo:7
        env:
          MONGO_INITDB_ROOT_USERNAME: test
          MONGO_INITDB_ROOT_PASSWORD: test

      opensearch:
        image: opensearchproject/opensearch:2.11.0
        env:
          discovery.type: single-node
          OPENSEARCH_INITIAL_ADMIN_PASSWORD: Test1234!

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          cd server
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run linting
        run: |
          cd server
          black --check .
          flake8 .
          mypy .

      - name: Run tests with coverage
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
          MONGODB_URL: mongodb://test:test@localhost:27017
          OPENSEARCH_URL: https://admin:Test1234!@localhost:9200
        run: |
          cd server
          pytest --cov=app --cov-report=xml --cov-report=term

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./server/coverage.xml

  test-dashboard:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: dashboard/package-lock.json

      - name: Install dependencies
        run: |
          cd dashboard
          npm ci

      - name: Run linting
        run: |
          cd dashboard
          npm run lint

      - name: Run type checking
        run: |
          cd dashboard
          npm run type-check

      - name: Build
        run: |
          cd dashboard
          npm run build

      - name: Run tests
        run: |
          cd dashboard
          npm test

  build-and-push:
    needs: [test-backend, test-dashboard]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push backend
        uses: docker/build-push-action@v4
        with:
          context: ./server
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/cybersentinel-manager:latest
            ${{ secrets.DOCKERHUB_USERNAME }}/cybersentinel-manager:${{ github.sha }}
          cache-from: type=registry,ref=${{ secrets.DOCKERHUB_USERNAME }}/cybersentinel-manager:buildcache
          cache-to: type=registry,ref=${{ secrets.DOCKERHUB_USERNAME }}/cybersentinel-manager:buildcache,mode=max

      - name: Build and push dashboard
        uses: docker/build-push-action@v4
        with:
          context: ./dashboard
          file: ./dashboard/Dockerfile.prod
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/cybersentinel-dashboard:latest
            ${{ secrets.DOCKERHUB_USERNAME }}/cybersentinel-dashboard:${{ github.sha }}

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'

    steps:
      - name: Deploy to staging
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.STAGING_HOST }}
          username: ${{ secrets.STAGING_USER }}
          key: ${{ secrets.STAGING_SSH_KEY }}
          script: |
            cd /opt/cybersentinel
            docker-compose -f docker-compose.staging.yml pull
            docker-compose -f docker-compose.staging.yml up -d
            docker-compose -f docker-compose.staging.yml exec -T manager python -m alembic upgrade head

  deploy-production:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
      - name: Deploy to production
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.PROD_HOST }}
          username: ${{ secrets.PROD_USER }}
          key: ${{ secrets.PROD_SSH_KEY }}
          script: |
            cd /opt/cybersentinel
            docker-compose -f docker-compose.prod.yml pull
            docker-compose -f docker-compose.prod.yml up -d
            docker-compose -f docker-compose.prod.yml exec -T manager python -m alembic upgrade head
```

**Implementation Steps:**
1. Create `.github/workflows/` directory
2. Implement CI/CD pipeline YAML
3. Add code quality checks (black, flake8, mypy, eslint)
4. Configure secrets in GitHub
5. Set up staging and production environments
6. Add deployment notifications (Slack/Email)
7. Implement rollback mechanism

**Estimated Time:** 2-3 days

---

## Phase 5: Reporting & Analytics (Priority: MEDIUM)

### 5.1 Incident Trends Dashboard ðŸ”„ NOT STARTED

**Objective:** Visualize incident trends over time.

**Dashboard Component:** `dashboard/src/pages/Analytics.tsx`

```typescript
// Dashboard with time-series charts for incident trends

import React, { useState } from 'react'
import { useQuery } from '@tanstack/react-query'
import {
  LineChart, Line, BarChart, Bar, PieChart, Pie,
  XAxis, YAxis, CartesianGrid, Tooltip, Legend,
  ResponsiveContainer
} from 'recharts'
import { api } from '@/lib/api'

export default function AnalyticsPage() {
  const [timeRange, setTimeRange] = useState('7d') // 1d, 7d, 30d, 90d

  const { data: trendData } = useQuery({
    queryKey: ['incident-trends', timeRange],
    queryFn: () => api.getIncidentTrends(timeRange)
  })

  const { data: topViolators } = useQuery({
    queryKey: ['top-violators', timeRange],
    queryFn: () => api.getTopViolators(timeRange)
  })

  const { data: dataTypeStats } = useQuery({
    queryKey: ['data-type-stats', timeRange],
    queryFn: () => api.getDataTypeStatistics(timeRange)
  })

  return (
    <div className="space-y-6">
      <h1>Security Analytics</h1>

      {/* Time Range Selector */}
      <div className="flex gap-2">
        {['1d', '7d', '30d', '90d'].map(range => (
          <button
            key={range}
            onClick={() => setTimeRange(range)}
            className={range === timeRange ? 'active' : ''}
          >
            {range}
          </button>
        ))}
      </div>

      {/* Incident Trends Chart */}
      <div className="card">
        <h2>Incident Trends</h2>
        <ResponsiveContainer width="100%" height={300}>
          <LineChart data={trendData}>
            <CartesianGrid strokeDasharray="3 3" />
            <XAxis dataKey="date" />
            <YAxis />
            <Tooltip />
            <Legend />
            <Line type="monotone" dataKey="total" stroke="#3b82f6" />
            <Line type="monotone" dataKey="blocked" stroke="#ef4444" />
            <Line type="monotone" dataKey="alerts" stroke="#f59e0b" />
          </LineChart>
        </ResponsiveContainer>
      </div>

      {/* Top Violators */}
      <div className="card">
        <h2>Top Violators</h2>
        <ResponsiveContainer width="100%" height={300}>
          <BarChart data={topViolators}>
            <CartesianGrid strokeDasharray="3 3" />
            <XAxis dataKey="agent_name" />
            <YAxis />
            <Tooltip />
            <Bar dataKey="violations" fill="#ef4444" />
          </BarChart>
        </ResponsiveContainer>
      </div>

      {/* Data Type Distribution */}
      <div className="card">
        <h2>Data Type Distribution</h2>
        <ResponsiveContainer width="100%" height={300}>
          <PieChart>
            <Pie
              data={dataTypeStats}
              dataKey="count"
              nameKey="type"
              cx="50%"
              cy="50%"
              label
            />
            <Tooltip />
          </PieChart>
        </ResponsiveContainer>
      </div>
    </div>
  )
}
```

**Backend API:** `server/app/api/v1/analytics.py`

```python
from fastapi import APIRouter, Depends, Query
from typing import List, Dict, Any
from datetime import datetime, timedelta
from app.core.opensearch import opensearch_client

router = APIRouter(prefix="/analytics", tags=["analytics"])

@router.get("/incident-trends")
async def get_incident_trends(time_range: str = Query("7d")):
    """Get incident trends over time"""
    days = int(time_range[:-1])
    start_date = datetime.utcnow() - timedelta(days=days)

    # OpenSearch aggregation query
    query = {
        "query": {
            "range": {
                "@timestamp": {
                    "gte": start_date.isoformat()
                }
            }
        },
        "aggs": {
            "trends": {
                "date_histogram": {
                    "field": "@timestamp",
                    "interval": "1d"
                },
                "aggs": {
                    "total": {"value_count": {"field": "event_id"}},
                    "blocked": {
                        "filter": {"term": {"blocked": True}},
                        "aggs": {"count": {"value_count": {"field": "event_id"}}}
                    }
                }
            }
        }
    }

    result = await opensearch_client.search(index="cybersentinel-events-*", body=query)

    # Transform results
    trends = []
    for bucket in result["aggregations"]["trends"]["buckets"]:
        trends.append({
            "date": bucket["key_as_string"],
            "total": bucket["doc_count"],
            "blocked": bucket["blocked"]["count"]["value"]
        })

    return trends

@router.get("/top-violators")
async def get_top_violators(time_range: str = Query("7d"), limit: int = 10):
    """Get top policy violators"""
    # Similar aggregation query grouping by agent_id
    pass

@router.get("/data-type-stats")
async def get_data_type_statistics(time_range: str = Query("7d")):
    """Get distribution of detected data types"""
    # Aggregation query on classification.type
    pass
```

**Implementation Steps:**
1. Create analytics API endpoints
2. Implement OpenSearch aggregation queries
3. Create Analytics dashboard page
4. Add time range selectors
5. Implement chart components
6. Add export functionality (CSV/PDF)

**Estimated Time:** 3-4 days

### 5.2 Scheduled Reporting System ðŸ”„ NOT STARTED

**Objective:** Automated report generation and distribution.

**File:** `server/app/services/report_service.py`

```python
"""
Scheduled Reporting Service
Generate and distribute periodic reports
"""

import schedule
import asyncio
from datetime import datetime, timedelta
from jinja2 import Template
import aiofiles
from typing import List, Dict, Any

class ReportService:
    """Generate and distribute reports"""

    def __init__(self):
        self.report_templates = {}
        self._load_templates()

    def _load_templates(self):
        """Load report templates"""
        # HTML templates for reports
        pass

    async def generate_daily_report(self):
        """Generate daily incident report"""
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=1)

        # Gather statistics
        stats = await self._gather_statistics(start_date, end_date)

        # Generate report
        report_html = await self._render_report("daily", stats)

        # Save report
        report_path = f"reports/daily_{end_date.strftime('%Y%m%d')}.html"
        await self._save_report(report_path, report_html)

        # Distribute report
        await self._distribute_report(report_html, "daily")

    async def generate_weekly_report(self):
        """Generate weekly summary report"""
        pass

    async def generate_monthly_report(self):
        """Generate monthly executive report"""
        pass

    async def _gather_statistics(self, start_date, end_date) -> Dict:
        """Gather report statistics"""
        from app.core.opensearch import opensearch_client

        query = {
            "query": {
                "range": {
                    "@timestamp": {
                        "gte": start_date.isoformat(),
                        "lte": end_date.isoformat()
                    }
                }
            },
            "aggs": {
                "total_events": {"value_count": {"field": "event_id"}},
                "by_severity": {"terms": {"field": "event.severity"}},
                "by_type": {"terms": {"field": "event.type"}},
                "pii_types": {"terms": {"field": "classification.type"}},
                "top_agents": {"terms": {"field": "agent.id", "size": 10}}
            }
        }

        result = await opensearch_client.search(
            index="cybersentinel-events-*",
            body=query
        )

        return {
            "total_events": result["aggregations"]["total_events"]["value"],
            "by_severity": result["aggregations"]["by_severity"]["buckets"],
            "by_type": result["aggregations"]["by_type"]["buckets"],
            "pii_types": result["aggregations"]["pii_types"]["buckets"],
            "top_agents": result["aggregations"]["top_agents"]["buckets"]
        }

    async def _render_report(self, template_name: str, data: Dict) -> str:
        """Render report HTML"""
        template = self.report_templates[template_name]
        return template.render(**data)

    async def _distribute_report(self, report_html: str, report_type: str):
        """Distribute report via email"""
        from app.core.observability import AlertManager

        alert_manager = AlertManager()
        # Send via email
        pass

    def schedule_reports(self):
        """Schedule periodic reports"""
        # Daily at 6 AM
        schedule.every().day.at("06:00").do(
            lambda: asyncio.create_task(self.generate_daily_report())
        )

        # Weekly on Monday at 8 AM
        schedule.every().monday.at("08:00").do(
            lambda: asyncio.create_task(self.generate_weekly_report())
        )

        # Monthly on 1st at 9 AM
        schedule.every().month.at("09:00").do(
            lambda: asyncio.create_task(self.generate_monthly_report())
        )
```

**Implementation Steps:**
1. Create report service
2. Design HTML report templates
3. Implement PDF generation (using weasyprint or reportlab)
4. Add email distribution
5. Create report scheduling system
6. Add report archive/storage
7. Create dashboard for viewing past reports

**Estimated Time:** 4-5 days

---

## Phase 6: Integration (Priority: LOW)

### 6.1 SIEM Integration ðŸ”„ NOT STARTED

**Objective:** Send events to SIEM platforms.

**File:** `server/app/integrations/siem.py`

```python
"""
SIEM Integration
Send events to ELK, Splunk, Wazuh
"""

from typing import Dict, Any
import aiohttp

class SIEMIntegration:
    """Base class for SIEM integrations"""

    async def send_event(self, event: Dict[str, Any]):
        """Send event to SIEM"""
        raise NotImplementedError

class ElasticsearchSIEM(SIEMIntegration):
    """Elasticsearch / ELK Stack integration"""

    def __init__(self, url: str, api_key: str):
        self.url = url
        self.api_key = api_key

    async def send_event(self, event: Dict[str, Any]):
        """Send event to Elasticsearch"""
        async with aiohttp.ClientSession() as session:
            headers = {"Authorization": f"ApiKey {self.api_key}"}
            async with session.post(
                f"{self.url}/dlp-events/_doc",
                json=event,
                headers=headers
            ) as response:
                return await response.json()

class SplunkSIEM(SIEMIntegration):
    """Splunk HEC integration"""

    def __init__(self, url: str, token: str):
        self.url = url
        self.token = token

    async def send_event(self, event: Dict[str, Any]):
        """Send event to Splunk via HEC"""
        async with aiohttp.ClientSession() as session:
            headers = {"Authorization": f"Splunk {self.token}"}
            payload = {
                "event": event,
                "sourcetype": "dlp:event",
                "index": "dlp"
            }
            async with session.post(
                f"{self.url}/services/collector/event",
                json=payload,
                headers=headers
            ) as response:
                return await response.json()
```

**Implementation Steps:**
1. Implement SIEM integrations (ELK, Splunk, Wazuh)
2. Add configuration for SIEM endpoints
3. Create event transformation for each SIEM format
4. Add retry logic and buffering
5. Implement health checks
6. Add SIEM integration tests

**Estimated Time:** 3-4 days

---

## Implementation Timeline

### Priority 1 (Weeks 1-2): Feature Expansion
- Multi-channel DLP (Email, Upload, Network): 10 days
- Policy templates (GDPR, HIPAA, PCI-DSS): 3 days
- Enhanced actions: 4 days

**Total:** ~17 days (3.5 weeks)

### Priority 2 (Weeks 3-4): Deployment & Reporting
- CI/CD pipeline: 3 days
- Incident trends dashboard: 4 days
- Scheduled reporting: 5 days

**Total:** ~12 days (2.5 weeks)

### Priority 3 (Week 5): Integration
- SIEM integration: 4 days
- Testing and refinement: 3 days

**Total:** ~7 days (1.5 weeks)

**Overall Timeline:** 7-8 weeks to complete all phases

---

## Success Metrics

### Phase 3 Success Criteria:
- [ ] Email monitoring operational with 95%+ detection rate
- [ ] File upload scanning integrated in <500ms
- [ ] Policy templates available for GDPR, HIPAA, PCI-DSS
- [ ] All action types functional and tested
- [ ] Dashboard supports CSV/PDF export

### Phase 4 Success Criteria:
- [ ] CI/CD pipeline runs on every commit
- [ ] Test coverage >85%
- [ ] Automated deployment to staging/production
- [ ] Zero-downtime deployments
- [ ] Rollback capability tested

### Phase 5 Success Criteria:
- [ ] Analytics dashboard shows real-time trends
- [ ] Daily/weekly/monthly reports generated automatically
- [ ] Reports distributed via email
- [ ] Historical report archive accessible

### Phase 6 Success Criteria:
- [ ] Events flowing to ELK/Splunk
- [ ] SIEM integration tested end-to-end
- [ ] Cloud storage monitoring operational
- [ ] Integration documentation complete

---

## Deployment Checklist

Before production deployment:

### Security:
- [ ] All secrets in environment variables
- [ ] TLS/SSL certificates configured
- [ ] Database encryption at rest enabled
- [ ] API rate limiting enabled
- [ ] Input validation on all endpoints
- [ ] CORS configured correctly

### Performance:
- [ ] Load testing completed (1000+ req/s)
- [ ] Database indexes optimized
- [ ] Redis caching configured
- [ ] CDN for dashboard assets
- [ ] Compression enabled

### Monitoring:
- [ ] Prometheus metrics exposed
- [ ] Grafana dashboards configured
- [ ] Alert rules defined
- [ ] Log aggregation working
- [ ] Health check endpoints active

### Compliance:
- [ ] GDPR data retention configured
- [ ] Audit logging enabled
- [ ] User consent tracking
- [ ] Data export capability
- [ ] Incident response plan documented

---

## Support & Maintenance

### Weekly Tasks:
- Review security alerts
- Check system health metrics
- Update dependencies
- Review and archive old logs

### Monthly Tasks:
- Security audit
- Performance optimization
- Backup verification
- Update documentation

### Quarterly Tasks:
- Penetration testing
- Disaster recovery drill
- Compliance review
- Feature prioritization

---

**Document Version:** 1.0
**Last Updated:** 2025-01-13
**Next Review:** 2025-02-13

---

## Appendix: Quick Reference

### Running Tests
```bash
# Backend tests
cd server
pytest -v --cov=app

# Dashboard tests
cd dashboard
npm test

# Integration tests
docker-compose -f docker-compose.test.yml up
pytest tests/integration/
```

### Deployment Commands
```bash
# Staging deployment
docker-compose -f docker-compose.staging.yml up -d

# Production deployment
docker-compose -f docker-compose.prod.yml up -d

# Rollback
docker-compose -f docker-compose.prod.yml down
docker-compose -f docker-compose.prod.yml up -d --no-deps manager
```

### Monitoring URLs
```
Prometheus: http://localhost:9090
Grafana: http://localhost:3001
Kibana: http://localhost:5601
```

---

**For questions or issues, contact:** dlp-team@company.com
